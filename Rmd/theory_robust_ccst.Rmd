### Pseudo Linearization

@Cha11 and @Cha14 deal with the estimation of the MSE of robust area predictors
  in the context of Small Area Estimation. In this section I review their
  results. Later in section ? I will, firt, adapt their findings to estimate the
  MSE of the robustified Fay Herriot model, and second use the linearization of
  robust mixed models to derive a fixed point algorithm to find solutions for
  the model parameters.

The central idea is to formulate the RBLUP as wigthed sum of the response vector:

$$
\si{\theta}^{RBLUP} = \sum_{j \in s} \sij{w}^{RBLUP} \sij{y} = \Paran{\mat{w}_{is}^{RBLUP}}^\top \mat{y}_s
$$

where

$$
\Paran{\mat{w}_{is}^{RBLUP}}^\top = N_i^{-1} \Paran{\mat{1}_s^\top + (N_i - n_i) \Paran{\bar{x}_{ir}^\top \mat{A}_s + \bar{z}_{ir}^\top \mat{B}_s \Paran{\mat{I}_s - \mat{X}_s \mat{A}_s }}}
$$

  and

$$
\mat{A}_s = \Paran{\mat{X}_s^\top \mat{V}_s^{-1} \mat{U}_s^\frac{1}{2} \mat{W}_{1s} \mat{U}_s^{-\frac{1}{2}} \mat{X}_s }^{-1} \mat{X}_s^\top \mat{V}_s^{-1} \mat{U}_s^\frac{1}{2} \mat{W}_{1s} \mat{U}_s^{-\frac{1}{2}}
$$

with

$$
\mat{W}_{1s} = \Diag{w_j}_{n \times n}
$$

and

$$
w_{1j} = \frac{\psi\Paran{ U_j^{-\frac{1}{2}} \Paran{ y_j - x_j^\top\hat{\beta}^\psi } }}{ U_j^{-\frac{1}{2}} \Paran{ y_j - x_j^\top\hat{\beta}^\psi }}
$$

$$
  \mat{B}_s =
  \Paran{
    \mat{Z}_s^\top \mat{V}_{es}^{-\frac{1}{2}} \mat{W}_{2s} \mat{V}_{es}^{-\frac{1}{2}} \mat{Z}_s +
      \mat{V}_u^{-\frac{1}{2}} \mat{W}_{3s} \mat{V}_u^{-\frac{1}{2}}
    }^{-1}
\mat{Z}_s^\top \mat{V}_e^{-\frac{1}{2}} \mat{W}_{2s} \mat{V}_e^{-\frac{1}{2}}
$$

  with $\mat{W}_{2s}$ as diagonal matrix with ith component:

  $$
  w_{2i} =
  \frac{
    \psi\Paran{\Paran{\sigma^\psi_{e, i}}^{-1} \Paran{y_i - x_i^\top \hat{\beta}^\psi - \hat{u}^\psi_i}}
  }{
    \Paran{\sigma^\psi_{e, i}}^{-1} \Paran{y_i - x_i^\top \hat{\beta}^\psi - \hat{u}^\psi_i}
  }
$$

and with $\mat{W}_{3s}$ as $\Paran{m \times m}$ diagonal matrix with ith component:

  $$
  w_{3i} = \frac{
    \psi\Paran{\Paran{\sigma_u^\psi}^{-1} \hat{u}^\psi_i}
  }{
    \Paran{\sigma_u^\psi}^{-1} \hat{u}^\psi_i
  }
$$

This all assumes known variance parameters. When the variance parameters are
unknown, they are estimated and instead of $\mat{w}_{is}^{RBLUP}$ we have to use
$\mat{w}_{is}^{REBLUP}$. Then the estimator of the conditional MSE is given by:

$$
\widehat{MSE}\Paran{\si{\widehat{\theta}}^{REBLUP}} = 
  \widehat{\Exp{V}}\Paran{\si{\widehat{\theta}}^{REBLUP}} + 
  \widehat{\Exp{B}}\Paran{\si{\widehat{\theta}}^{REBLUP}}^2
$$

$$
\widehat{\Exp{V}}\Paran{\si{\widehat{\theta}}^{REBLUP}} =
  N_i^{-2} \sum_{j \in s} \Paran{a_{ij}^2 + \Paran{N_i - n_i} n^{-1}} \lambda_j^{-1}\Paran{y_j - \hat{\mu}_j}^2
$$

with 

$$
a_{ij} = N_i w_{ij}^{REBLUP} - I\Paran{j \in i}
$$

and

$$
\widehat{\Exp{B}}\Paran{\si{\widehat{\theta}}^{REBLUP}} =
  \sum_{j \in s} w_{ij}^{REBLUP} \hat{\mu}_j - N_i^{-1} \sum_{j \in \Paran{r_i \cup s_i}} \hat{\mu}_j
$$

Note that $\hat{\mu}_j$ is an unbiased estimator of the the conditional expectation $\mu_j = \Exp{E}\Paran{y_j | \mat{x}_j, \mat{\re}^\psi}$. $\lambda_j = 1 - 2\phi_{jj} + \sum_{k \in s}\phi^2_{kj}$.




