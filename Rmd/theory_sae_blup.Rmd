# Small Area Prediction using Linear Mixed Models
\label{sec:blup}

This Section gives a general overview of linear mixed models and the best linear
unbiased prediction (BLUP) and empirical BLUP (EBLUP). The unit and area level
models introduced here are based on this class of models and the robust
methodology is based on a robustified EBLUP. The original model dates back to
@Hen50 and an early comprehensive overview can be found in @Sea71. @Jia06 review
liner mixed models in the context of SAE and the main results can also be found
in @Rao03 and @Rao15.

## Linear Mixed Models

Following @Rao15[98 ff] a linear mixed model can be expressed by:
\empty{
\begin{align}
\label{eq:lmm}
\mat{y} = \mat{X}\pmat{\beta} + \mat{Z}\mat{\re} + \mat{e}
\end{align}
}where
$\mat{y}$ is the $(n \times 1)$ vector of response values; $\mat{X}$ is a
$(n \times P)$ matrix containing auxiliary and deterministic information;
$\pmat\beta$ is the $(P \times 1)$ vector of regression coefficients; $\mat{Z}$ is a
known matrix and $\mat{\re}$ is a vector of random effects, such that $\mat{Z\re}$ is
of dimension $(n \times 1)$; $\mat{e}$ is the $(n \times 1)$ vector of model
errors. Note that $\mat{\re}$ and $\mat{e}$ are both random variables where the
basic assumption is that both have mean zero and finite variances. Furthermore,
they are assumed to be independent.

If, in addition, $\mat{\re}$ and $\mat{e}$ are assumed to follow a normal
distribution, the model is called a Gaussian Linear Mixed Model \citep{Jia06}. The
distribution of $\mat{y}$ can then be derived as a multivariate normal of the form:
\empty{
\begin{align*}
\mat{y} &\sim \Distr{N}\Paran{\mat{X}\pmat{\beta}, \mat{V}} \\
\mat{y} | \mat{\re} &\sim \Distr{N}\Paran{\mat{X}\pmat{\beta} + \mat{Z}\mat{\re}, \mat{V}_e}
\end{align*}
}where
$\mat{V} = \mat{Z}\mat{V}_\re\mat{Z}^\top + \mat{V}_e$ with $\mat{V}_\re$
and $\mat{V}_e$ being the variance covariance matrices of $\mat{\re}$ and
$\mat{e}$, respectively. Such variance structure typically depend on some unknown
dispersion parameters. To be more precise: $\mat{V}_\re =
\mat{V}_\re(\pmat{\delta}_\re)$ and $\mat{V}_e = \mat{V}_e(\pmat{\delta}_e)$ such that
$\mat{V} = \mat{V}(\pmat{\delta})$ with $\pmat{\delta} = (\pmat{\delta}_\re, \pmat{\delta}_e)$.

## Best Linear Unbiased Prediction

Here I also follow the results given by @Rao15[98 ff]. Given the model
\eq{eq:lmm} above in SAE problems we are generally interested in estimating the
expected value of $\mat{y}$ given $\mat{\re}$:
\empty{
\begin{align*}
\pmat{\mu} = \mat{l}^\top \pmat{\beta} + \mat{m}^\top \mat{\re}
\end{align*}
}for specified values of $\mat{l}$ and $\mat{m}$. An
estimator for $\pmat{\mu}$ can be obtained by replacing $\pmat{\beta}$ and
$\mat{\re}$ with suitable estimators. For known variance components,
$\pmat\delta$, the best linear unbiased estimator (BLUE) is given by:
\empty{
\begin{align}
\label{eq:blue}
\tilde{\pmat{\beta}} = \tilde{\pmat{\beta}}(\pmat{\delta}) = \Paran{\mat{X}^\top \mat{V}^{-1}\mat{X}}^{-1} \mat{X}^\top \mat{V}^{-1}\mat{y}
\end{align}
}and the BLUP for $\mat{\re}$ by:
\empty{
\begin{align}
\label{eq:blupre}
\tilde{\mat{\re}} = \tilde{\mat{\re}}(\pmat{\delta}) = \mat{V}_\re\mat{Z}^\top\mat{V}^{-1}\Paran{\mat{y} - \mat{X}\tilde{\pmat{\beta}}}
\end{align}
}such that the BLUP estimator for $\pmat\mu$ can be stated as:
\empty{
\begin{align}
\label{eq:blup}
\tilde{\pmat{\mu}} = \tilde{\pmat{\mu}}(\pmat{\delta}) = \mat{l}^\top \tilde{\pmat{\beta}} + \mat{m}^\top \tilde{\mat{\re}}.
\end{align}
}The BLUP \eq{eq:blup} of $\pmat{\mu}$ depends on known variance components $\pmat\delta$.
These values are typically unknown in applications and are themselves subject to
estimation. If we replace $\pmat\delta$ with a suitable estimator, $\hat{\pmat{\delta}}$, 
the empirical BLUP (EBLUP) obtained is:
\empty{
\begin{align}
\label{eq:eblup}
\hat{\pmat\mu} = \hat{\pmat\mu}(\hat{\pmat\delta}) = \mat{l}^\top \hat{\pmat\beta} + \mat{m}^\top \hat{\mat{\re}}
\end{align}
}where $\hat{\pmat\beta} = \tilde{\pmat\beta}(\hat{\pmat\delta})$ and
$\hat{\mat\re} = \tilde{\mat\re}(\hat{\pmat\delta})$. To estimate $\pmat\delta$
a variety of estimators have been proposed. Commonly used estimators are based
on maximum likelihood (ML) and restricted maximum likelihood (REML). The
standard procedures in the literature are not directly feasible for the robust
estimators introduced in Chapter \ref{chap:rfh}. Instead different algorithms
are proposed. For a detailed discussion of the estimation of the variance
parameters see also @Jia06 and the literature quoted there.

## Mean Squared Prediction Error
\label{sec:eblup_mspe}
One of the main reasons for relying on small area methods is to reduce the mean 
squared error of domain predictions. Since domain predictions under a linear 
mixed model are derived as the EBLUP, we are generally interested in the mean 
squared prediction error (MSPE) of the EBLUP. Note that here I present results
directly for the EBLUP instead of the BLUP since the latter has little practical
relevance and the results are mainly needed to give a comprehensive context in 
which the existing literature can be extended. In general the estimation of the 
MSPE can be identified as one of the challenging problems in model based SAE
\citep{Pfe13}. Two approaches are taken in the literature: The analytical 
identification of the MSPE and different resampling strategies. More detailed 
reviews of these strategies can be found in @Rao03[p. 95 ff], @Jia06[p. 13 ff]
and in @Dat09.

Early results can be found in @Kac84 who propose an approximation to the MSPE
of the EBLUP of a Gaussian linear mixed model. In particular they show that:
\empty{
\begin{align}
\label{eq:mspe_eblup}
MSPE(\hat{\pmat\mu}) = MSPE(\pmat\mu) + \Exp{E}\Paran{\hat{\pmat\mu} - \pmat\mu}^2.
\end{align}
}The MSPE of $\mu$ can be decomposed such that
\empty{
\begin{align*}
MSPE(\pmat\mu) &= g_1(\pmat\delta) + g_2(\pmat\delta)
\end{align*}
}\citep[98 ff]{Rao03}  where
\empty{
\begin{align*}
g_1(\pmat\delta) &= \mat{m}^\top\Paran{\mat{V}_\re - \mat{V}_\re\mat{Z}^\top\mat{V}^{-1}\mat{Z}\mat{V}_\re}\mat{m}\\
g_2(\pmat\delta) &= \mat{d}^\top \Paran{\mat{X}^\top\mat{V}^{-1}\mat{X}} \mat{d}
\end{align*}
}with $\mat{d}^\top = \mat{l}^\top - \mat{b}^\top\mat{X}$ and $\mat{b}^\top =
\mat{m}^\top\mat{V}_\re\mat{Z}^\top\mat{V}^{-1}$. The second term in equation
\eq{eq:mspe_eblup} was approximated by @Kac84 using a Taylor series
approximation. @Pra90 have used instead the approximation by
\empty{
\begin{align*}
\Exp{E}\Paran{\hat{\pmat\mu} - \pmat\mu}^2 &\approx g_3(\pmat\delta) \\
\end{align*}
}where
\empty{
\begin{align*}
g_3(\pmat\delta) &= \tr\Paran{\frac{\partial \mat{b}^\top}{\partial \pmat\delta} \mat{V}\Paran{\frac{\partial \mat{b}^\top}{\partial \pmat\delta}}\mat{V}_{\hat{\pmat\delta}}}
\end{align*}
}with $\mat{V}_{\hat{\pmat\delta}}$ being the asymptotic covariance matrix of 
$\hat{\pmat\delta}$. The derived MSPE depends on the unknown parameter vector 
$\pmat\delta$. @Pra90 used a moment estimator for $\pmat\delta$ and replaced the
above formulae so that an estimator of the MSPE can be defined as 
$\widehat{MSPE}(\hat{\pmat\mu}) = g_1(\hat{\pmat\delta}) + 
g_2(\hat{\pmat\delta}) + g_3(\hat{\pmat\delta})$. @Dat00 extended this approach 
for a wider range of models in SAE including ML and REML estimators. An overview
and comparison of these methods can be found in @Dat05; their presentation
focuses on MSPE estimators using area level models.

A different approach has been taken by @Cha11 which is to define the EBLUP as a
weighted sum of the sampled values and to derive an MSPE estimator under the
assumption of independence between weights and sampled values. An advantage of
this method is its wide applicability as the approach is not restricted to
predictions under linear mixed models but extends to any predictor which can be
represented as a weighted sum of the sampled values. The approach has been extended in
@Cha14 to robust methods in SAE and is of special interest in deriving an MSPE
estimator for the methods proposed in this Thesis. Hence these results are
reviewed in more detail in Section \ref{sec:theory_sae_robust_mspe}.

As an alternative approach a wide range of different resampling strategies have
been proposed. @Jia02 introduce a jackknife method to estimate the MSPE in the 
context of longitudinal linear mixed models; this was modified by @Loh09 into a 
simpler form. Also important is the proposed double bootstrap method of @Hal06
and the block bootstrap of @Cha13 for mixed linear models. However of special 
interest are the methods in the context of robust predictions under linear mixed
models which is why the bootstrap methods of @Sin09, @Jio14, and @Mok13 are
reviewed in more detail in Section \ref{sec:theory_sae_robust_bootstrap}.
