# Performance of Mean Squared Prediction Error
\label{sec:results_mse}

In Section (?) some insights on the Monte Carlo MSPE and bias have been gained. 
In addition to the prediction of a target statistic we are of course also 
interested in a measure of uncertainty with respect to this quantity. To this 
extend two MSPE estimators have been proposed in Section (?): an adaption of the
parameteric bootstrap proposed by @Sin09 and a MSPE estimator based on the 
pseudolinerisation approach by @Cha11. In the following these two MSPE estimator
are compared in different simulation settings.

In the proviously conducted simulation study we saw that the simulation setting
revealed advantages and disadvantages of the robust methods. With respect to the
MSPE estimation the scenarios are chosen to be more confirmatory in terms of the
underlying model. However the computational effort with respect to the boostrap
estimator is relatively high which makes it necessary to restrict the number of
scenarios to a minimum. Thus for each model -- RFH, RSFH, RTFH, and RSTFH --
data is generated under the correct model and only a non\hyp{}outlier with a
outlier scenario is compared.

Also the results for the non\hyp{}robust methods are ommitted to reduce the
number of necessary comparissons. In principle a comparisson with established
methods like the MSPE estimator from @Pra90 would be valuable. However the MSPE
estimators associated to the non\hyp{}robust predictors are diverse -- see the
coresponding review in Section (?). 

In the following Section (?) the simulation settings for each model are
described in detail. A presentation of the results can be found in Section (?);
followed by a discussion of the results in the context of the existing
literature in Section (?).

## Simulation Scenarios

In the simulation study each model is fitted on data generated using the
coresponding model which can be represented in general form as:
$$
y_{it} = 100 + 5x_i + \mat{z}_{it}^\top\mat{u} + e_{it}
$$
with $i = 1, \dots, D$ and $t = 1, \dots, T$. The regressor is defined as
before: $x_i = \frac{i}{2D} + 1$; and the sampling error structure is the same
for all scenarios: $e_{it} \sim \Distr{N}(0, \sige)$ with $\sigma_{eit}^2 =
\sige$ and $\sige = \frac{4 (i - 1)}{D - 1} + 2$. Furthermore it is distinguised
between:

- RFH-(0): $D = 40$ and $T = 1$; $\mat{z}_{it}^\top\mat{u} = u_i$ with $u_i \sim
\Distr{N}(0, 9)$.
- RSFH-(0): $D = 40$ and $T = 1$; $\mat{z}_{it}^\top\mat{u} = u_{1i}$ with
$u_{1i} \sim SAR(1)$ where $\rho_1 = 0.5$ and $\sigma_1^2 = 9$.
- RTFH-(0): $D = 40$ and $T = 10$; $\mat{z}_{it}^\top\mat{u} = u_{0i} + u_{2it}$
with $u_{0i} \sim \Distr{N}(0, 9)$ and $u_{2it} \sim AR(1)$ where $\rho_2 = 0.5$
and $\sigma_2^2 = 9$.
- RSTFH-(0): $D = 40$ and $T = 10$; $\mat{z}_{it}^\top\mat{u} = u_{1i} +
u_{2it}$ with $u_{1i} \sim SAR(1)$ where $\rho_1 = 0.5$ and $\sigma_1^2 = 9$ and
$u_{2it} \sim AR(1)$ where $\rho_2 = 0.5$ and $\sigma_2^2 = 9$.

\noindent Here *(0)* is used to denote the non\hyp{}outlier scenario. For each
of these scenarios one outlier scenario is considered which is denoted by *(u)*
where we replace the random effect for the outlying domains:

- *-(u): $\mat{z}_{it}^\top\mat{u} = u_{i}$ with $u_i \sim \Distr{N}(9, 25)$
for all $i \in \{5, 15, 25, 35\}$. The set of outlying domains is chosen to
avoid an artificial scenario in combination with the choice for $\sige$.

## Quality Measures

In this setting we are interested in the performance of the MSPE estimators. To 
asses the quality of these estimators the RRMSE and RBIAS of the estimated root 
MSPE (RMSPE) are compared with the *true* values. Let $\widehat{RMSPE}^M_{ir}$ 
denote the estimated RMSPE for area $i$ in the $r$th Monte Carlo repetition 
using method $M$. $M$ is either the parameteric bootstrap referred to by BOOT or
the pseudolinearisation based approach which is refered to by CCT. Similar to 
the previous study predictions are made for the *current* time period, which is 
again defined as $t = T$ under the respective scenario. This also means that we 
evaluate only the RMSPE for these predictions. We can then define the RRMSE as:
$$
RRMSE_i^M = \sqrt{\frac{1}{R} \sum_{r = 1}^R \Paran{\frac{\widehat{RMSPE}^M_{ir} - RMSPE^M_{i}}{RMSPE^M_{i}}}^2}
$$
where we define the *true* RMSPE as the Monte Carlo RMSPE over all repetitions:
$$
RMSPE_i^M = \sqrt{\frac{1}{R} \sum_{r = 1}^R \Paran{\widehat{RMSPE}^M_{ir} - RMSPE^M_{i}}^2}
$$
Furthermore we have the RBIAS of the MSPE
estimator defined as:
$$
RBIAS_i^M = \frac{1}{R} \sum_{r = 1}^R \frac{\widehat{RMSPE}^M_{ir} - RMSPE^M_{i}}{RMSPE^M_{i}}
$$
These measures are computed for both MSPE estimators and for all robust 
predictions; the respective predictors are referred to by RFH, RSFH, RTFH, and 
RSTFH; and their bias\hyp{}corrected counterparts by RFH.BC, RSFH.BC, RTFH.BC,
and RSTFH.BC.

## Results

The main results of the simulation study can be found in Table
\ref{tab:mse_performace}. The following observations need to be discussed:

1. The CCT has not for all estimators a negative bias.
2. The CCT has a high MSE for the bias corrected predictors.
3. The advantage of the CCT compared to BOOT is small for the outlying domains.

\input{tabs/mse_template.tex}

The first observation is that the CCT has not a negative bias in all cases. 
However as we neglect the uncertainty due to the estimation of the variance 
parameters and assume independence between the weights and the response in the 
pseudolinear form we should expect an under estimation. For the RFH and RSFH in
the non\hyp{}outlier scenario this can be observed. Under the same scenario the
RTFH and RSTFH can have a positive bias since only the estimated MSPE for the
*current* time period is considered. In an analysis using all time periods the 
median RBIAS is indeed negative. The estimation for regular domains under the
outlier scenario can also have a positive bias. This happens since the variation
due to the random effect is higher when outliers are present. Thus the variance
parameters used to compute the MSPE for the regular observations are slightly
higher which is reflected in a higher estimate for the MSPE.

The second observation is the higher MSE of the CCT for the predictions with 
bias correction. The bias correction will bind the domain prediction. The choice
of the intervall in which predictions can be made leads -- in this simulation --
to the situation that more predictions than necessary are *bias corrected*. 
Since the weights for the bias correction -- see Section (?) -- are also used 
for estimating the MSPE more weight is given to the sampling variance. Leading
to more unstable results over all. This may be less relevant in an application
in which we can choose the width of the intervall. Also this effect was visible
in the model\hyp{}based simulation in Section (?).

The third observation we can make is that the CCT only has a small benefit in
terms of bias and MSE for the outlying observations. In general the results
regarding the performace of the MSPE estimators are very sensitive with respect
to the choices made for the model. In this setting the bias we can make in a
prediction is relatively small since the mean for outlying domains is shifted by
9 units compared to an overall intercept of 100. In the design\hyp{}based study
of Section (?) we will observe outlying observations which are multiple times
larger than the main body of observations. Hence the benefit in terms of bias
depends largely on the magnitude of the intercept of the outlying observations.
Furthermore it must be noted that the MSPE predictor will gain an advantage in
terms of MSE due to the reduced bias in the prediction. 

\begin{figure}[htbp]
\centering
\includegraphics[width = \textwidth]{figs/area_level_mse.pdf}
\caption[Estimated Root Mean Squared Prediction Error for the Bias Corrected Robust Spatial FH Model]{\label{fig:area_level_mse}Estimated Root Mean Squared Prediction Error for the Bias Corrected Robust Spatial FH Model. Compared are the bootstrap (BOOT.BC) and the pseudolinearisation\hyp{}based estimator (CCT.BC) with the Monte Carlo MSPE (MC.BC).}
\end{figure}

Figure \ref{fig:area_level_mse} also illustrates how the estimated RMSPE relates
to outliers. Note that observations 5, 15, 25, and 35 are outlying domains and 
that the sampling variances increase with $i$. What we see here is that the CCT 
for the bias corrected RSFH is able to follow the Monte Carlo MSPE better when 
the sampling variances are large. Since there is no information on the *true* 
variation in the outlier distribution we observe a better fit the more relevant 
the sampling variance is. This shows that these results largely depend on the
scenario in which the CCT method is applied. The bootstrap in contrast has no
means of mirroring the Monte Carlo MSPE for outlying observations regardless of
the scenario.


## Discussion

The performane of the MSPE estimators are overall promising. However the
concrete results strongly depend on the scenario settings. What we can generally
observe is that the CCT for the bias correction suffers under repeated sampling
when the prediction intervall of the correction is too conservative. For the
same reason we observe a correlation between the sampling variances and the
bias. The bootstrap in contrast showed very stable results -- approximately 10
per cent in terms of RRMSE -- over a variety of different settings. However this
method cannot capture the variation for outlying domains. In scenarios in which
the respective models repeatedly estimate close to zero variances the bootstrap
will underestimate the true variation since more weights is given to the linear
predictor. This setting has been -- in contrast to Section (?) -- avoided by
choosing larger values for the variance parameters in the above study.

The settings of this study have been chosen to be close to the approach taken by
@Cha14. Although a comparison is difficult to make because of the transition 
between area level and unit level models some similarities in the results can be
found. Most notably @Cha14 report values of the RRMSE for the bootstrap for the 
unit level REBLUP of approximately 10 per cent in scenarios with area level 
outliers and scenario without contamination. This result is very close to what 
we can observe in this study. In the specific scenario above the results for the
CCT show similar performance in terms of RRMSE to the bootstrap at least for the
not bias corrected predictions. In the study conducted by @Cha14 the CCT method 
shows not as good results as we can see here, however this can very well be due 
to the differences in the scenarios.
