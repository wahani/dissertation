# Motivation

The main endevour Small Area Estimation tries to solve is to produce *reliable 
predictions* of a *target statistic* for *small domains*. A *target statistic* 
can be a linear statistic such as a mean, a count, or quantiles; but it can take
other forms: e.g. an inequality measure like the Gini coefficient for poverty
mapping. Such statistics are produced for *small domains*, where domain refers
to specific groups, e.g. an industry sector, or groups defined by
socio\hyp{}economic characteristics. Because of its frequent application to
administrative data, domains are often defined by areas as a geographical unit.
They are small in the sense that they comprise few or no sampled units within
these domains. This has the effect that a direct estimation, i.e. an estimation
which only relies on the information available within domains, tends to be not
reliable. *Reliability* is here measured either by the variance or mean squared
error of the predictions.

Small Area Estimation tries to improve such domain predictions, often in terms
of mean squared error, by *borrowing strength* from other domains. This can 
happen by taking additional information from other data sources into account,
like census and register information. Also structures in the data, like spatial
or temporal correlation, can be exploited to improve a prediction.

The importance of the field can be explained by the increasing demand for 
reliable estimates by policy makers and in official statistics. Results may be 
used for fund allocation, health programs, agriculture, or poverty mapping to 
name only a few of the fields of application. Traditionally such estimates rely 
on survey data; but as the target domains become more diverse, reliable 
estimates are connected to an increasing demand for sampled units within the 
domains. The conflict between the demand for more diverse domains as against the
cost and feasibility for larger samples is the factor that stimulates the
progress within the field as this supplies the mechanism for optimising the
ratio between sampled units and the reliability of estimates.


# Locating the Contents in the Field of Small Area Estimation

In the following I give a general overview of the field of Small Area Estimation
(SAE) necessary in order to accomodate this Thesis within the SAE field. @Rao03 
as well as @Rao15 give comprehensive overviews of the established methods and 
research published in this field. @Gho94, @Rao99, @Pfe02 and @Pfe13
focus on the status quo and main lines of discussion within the field at their
respective point in time.

In general small area methods are divided into two categories: design based and 
model based methods. Design based methods can be considered the traditional 
methodology for analysing survey data; a comprehensive overview of these methods
for SAE can be found in @Leh09. Design based methods comprise different direct
and indirect techniques. The Horvitz\hyp{}Thompson (HT) estimator by @Hor52 which
only uses sampled units within domains, synthetic regression estimates as well
as model assisted methods like generalized regression (GREG) estimators
- see @Saer92 for a discussion of these methods - are examples of such
estimators. These methods have in common that they incorporate
information of the sampling design into the estimation.

Conceptually design based and model based methods differ in that design based
methods are used to optimally estimate a target parameter of a fixed and finite 
population. Model based methods rely instead on the idea that an observed
sample is drawn from a population which is but one possible realisation of a 
*superpopulation* model, and it is the parameters of that superpopulation which 
are targeted. This difference leads to a trade-off when choosing between 
methods: model based methods can improve domain estimation in terms of variance 
even with small samples, however, they cannot be considered design unbiased. 
Design based methods on the other hand are design unbiased but have larger and 
possibly unacceptably high variances for small samples -- see @Leh09.

Model based methods can be further separated into area and unit level models. 
Observations which can be associated with a specific domain are referred to as 
units. These can be companies within an industry sector or individuals within a 
municipality. The area level describes models which use information on area 
level, i.e. direct estimates for domains. A situation in which these models are 
considered is when data can only be provided as aggregates due to factors such
as confidentiality. Also such methods may be useful in situations in which the 
computational effort is high -- e.g. when complex variance structures are 
combindes with large data sets.

One class of models in particular is favoured in different variations: mixed 
models. The Fay-Herriot (FH) model introduced by @Fay79 and the 
Battese\hyp{}Harter\hyp{}Fuller (BHF) model which was introduced by @Bat88 are 
the two basic models which are used respectively for area and unit level models.
Underlying this is the idea to use auxiliary information in a regression to
estimate a global conditional mean and add an extra component to capture the
domain specific difference from that global mean. This general idea can be found
in combination with different estimation methodologies, i.e. general linear
mixed models which are typically associated with best linear unbiased predictors
(BLUPs), empirical Bayes, and hierarchical Bayes. Although these different 
frameworks for estimation differ with respect to optimality criteria, 
equivalence of the derived estimators can be shown for special cases. 
A more general discussion of mixed models in SAE can be found in @Jia06. @Rao03 
and @Rao15 provide a comprehensive overview and comparison of the different 
frameworks.

A general property of model based methods is that a lot of their benefits in 
terms of efficiency rely on strong distributional assumptions. Hence it is not 
only in the field of SAE that robust methods have been exploited to reduce the 
negative effect of a potential violation of these assumptions. The general 
problem here is that single observations can have unwanted and overly large
impact on results. Such observations are typically called outliers. @Cha86 uses
the term *representative* outliers to describe observations which are correctly
recorded and can not be assumed to be unique in the population. 
Non\hyp{}representative outliers, on the other hand, may be best described as 
*not correctly recorded* and should be imputed or generally dealt with during 
the editing process of survey data.

To summarise robust methods in SAE it is necessary to distinguish between three 
different lines of discussion. Firstly, if the distributional assumption - often a
Gaussian distribution - appears to be implausible then intuition demands that it
be replaced. This often leads to the use of non-symmetric or heavy-tailed 
distributions for the model error or the random effect. Due to their flexibility
Bayesian modelling strategies are often used in this context; see for example
@Dat95 and @Bel06. Secondly, methods are applied which are *naturally* more 
  robust against outlying observations. @Cha06 and @Tza10 model a global
  conditional median, or more generally a quantile, instead of a mean. The third
  approach is to remain with the original model or method and *robustify* the
  estimation equations. In this context @Sin09 develop a robust EBLUP; @Bea09
  refer to a winzorisation of the Horvitz-Thompson estimator; and @Bea04
  introduces a robust extension to generalised regression estimation.

Given this background, in this Thesis I introduce extensions to the
Fay\hyp{}Herriot area level model using a model based perspective. More
precisely an EBLUP based approach is taken to derive predictions in a way that
makes it possible to model spatial and temporal covariance structures in the
random effects. These models are closely related to the results presented by
@Mar13, who introduce spatial and temporal extensions to the Fay-Herriot model.
However the methods in this Thesis are based on an estimation procedure which is
robust against outliers following the methodology introduced by @Sin09. The
extension introduced in the literature around robust EBLUPs (REBLUPs) have been
focused on unit level models, thus results especially to MSE estimation and bias
correction are extended to area level models.

# Reproducing the Results

# Outline

In addition reference is made to certain
supplementary material on software. @War15 introduces tools for simulation 
studies for the special case of small area estimation and @War16 implements the 
methods introduced in this Thesis. All results rely on these two packages; a 
more detailed discussion of these software packages can be found in Chapter 
\ref{chap:saeRobust} and \ref{chap:saeSim}.

The Thesis is seperated into three main parts. \ref{part:theory} *Theory*
introduces the underlying esimation methodology, i.e. linear mixed models, a
review of model based methods in Small Area Estimation as well as outlier robust
extensions within the field. Given these results extensions to existing
methodology is introduced in the form of a robustified Fay-Herriot estimator
with optional spatial and temporal correlated random effects. A special interest
lies in the concrete implementation of such (robust) estimators, and to meet
this focus several algorithms are proposed. (MSE, bias-correction)

\ref{part:implementation} *Implementation* introduces three main aspects: The 
verification that the implementation (in terms of software) is correct; How to 
evaluate the numerical accuracy and stability of the introduced algorithms; And 
which results to report to judge the quality of the numerical solution.

In \ref{part:results} *Results* the properties of the estimators are 
investigated in simulations and in the context of part 
\ref{part:implementation}. The numerical properties are devided into accuracy
and stability. Statistical properties are shown for the most reliable
implementations using model and design based simulation studies.



