# Area Level Models

In SAE mixed models are generally devided into area and unit level models. In
this section a review of some of the most important results is given with
respect to area level models. Section \ref{sec:unit_level} then presents the
basic unit level model.

## The Fay-Herriot Model

The basic area level model was introduced by @Fay79 and has been used to
predict the mean income of small areas using census data. The general setting is
that only information on the area level is available, i.e. direct estimates
for the domains. The model is then built around two stages. The first stage is
the sampling model:
$$
\si{\tilde{y}} = \si{\theta} + \si{e}
$$
where $\si{\tilde{y}}$ is a direct estimator for a statistic of interest,
$\si{\theta}$, for an area $i$ with $i = 1, \dots, D$ and $D$ being the number
of areas. The sampling error $\si{e}$ is assumed to be independent and normally
distributed with known variances $\sige$, i.e. $\si{e} \sim \Distr{N}(0,
\sige)$. The model is modified with a second stage, the linking model, by 
assuming a linear relationship between the true area statistic, $\si{\theta}$,
and some diterministic auxiliary variables $\si{\mat{x}}$:
$$
\si{\theta} = \si{\mat{x}}^\top \beta + \si{\re}
$$
where $\si{\mat{x}}$ is a $(P \times 1)$ vector containing area\hyp{}level
information for $P$ variables and $\beta$ is a ($P \times 1$) vector of
regression coefficients. The model errors $\si{\re}$ are assumed to be
independent and identically distributed following a normal distribution: $\re_i
\sim \Distr{N}(0, \sigre)$. Furthermore $e_i$ and $\re_i$ are assumed to be
independent. Combining the sampling and linking model leads to:
\begin{align}
\tilde{y}_i = \si{\mat{x}}^\top \beta + \re_i + e_i \label{eq:FH}
\end{align}

### Best Linear Unbiased Prediction

To obtain small area predictions under model \eq{eq:FH} it can be defined as a 
linear mixed model and a BLUP and EBLUP can be derived. Basically model 
\eq{eq:FH} can be directly viewed as a mixed model as it was introduced in
equation \eq{eq:lmm} where $\mat{Z} = \mat{I}_D$, $\mat{V}_\re =
\sigre\mat{I}_D$ and $\mat{V}_e = \diag(\sigma_{e, 1}^2, \dots, \sigma_{e,
D}^2)$ with $\mat{I}_D$ being a $(D\times D)$ identity matrix. The vector of
unknown variance parameter in this case is a scalar, such that $\delta = \sigre$
since $\sige$ is assumed to be known. The BLUP, defined in equation
\eq{eq:blup}, for the Fay-Herriot model can then be obtained by setting
$\tilde{\mu}_i = \tilde{\theta}_i^{FH}$,
$\mat{l}_i^\top = \mat{x}_i^\top$, $\mat{m}^\top_i = 1$ and $y_i = \tilde{y}_i$:
\empty{
\begin{align}
\tilde{\theta}_i^{FH} = 
\tilde{\theta}_i^{FH}(\sigma_\re^2) &= \si{\mat{x}}^\top\tilde{\beta} + \si{\tilde{\re}} \nonumber\\
                              &= \si{\mat{x}}^\top\tilde{\beta} + \frac{\sigma_\re^2}{\sigma_\re^2 + \sige} \Paran{\tilde{y}_i - \si{\mat{x}}^\top\tilde{\beta}} \nonumber\\
                              &= \gamma_i\tilde{y}_i + (1 - \gamma_i)\si{\mat{x}}^\top\tilde{\beta}
\end{align}
}with $\si{\gamma} = \sigre / (\sigre + \sige)$. The BLUP depends on the
variance parameter of the random effects, $\sigre$, which is unknown. To obtain
the EBLUP under the Fay-Herriot model we can replace the unknown parameter with
an estimate leading to:
$$
\si{\hat{\theta}}^{FH} = \si{\hat{\gamma}}\si{\tilde{y}} + \Paran{1 - \si{\hat{\gamma}}} \si{\mat{x}}^\top\hat{\beta}
$$
where $\si{\hat{\gamma}} = \hat{\sigma}_\re^2 / (\hat{\sigma}_\re^2 + \sige)$.
Note that the regression parameters are still estimated using the weighted least
squares estimator of equation \eq{eq:blue} with $\delta =
\hat{\sigma}_\re^2$. For the estimation of $\sigre$ different approaches exist.
@Fay79 proposed a moment estimator from which they derived an algorithm to 
  estimate $\sigre$. @Rao03[118-119] reviews several other ideas. Also based on 
  a moment estimator he derives the estimator also used in @Pra90. Both moment 
  estimators have the property that they do not rely on a normal distribution, 
  which is also true for the estimation of the regression coefficients. 
  Alternatively $\sigre$ can be estimated using maximum likelihood or 
  restricted maximum likelihood, which, in contrast, relies on the distributional 
  assumptions. For details see also @Dat00.


### Mean Squared Prediction Error
\label{sec:theory_sae_fh_mspe}

The MSPE of the EBLUP under the Fay-Herriot model, $\si{\hat{\theta}}^{FH}$, is
subject to several studies. However, it needs to be noted that, interestingly,
@Fay79 did not asses the qunatification of uncertainty associated with their
predictions. In principle the MSPE can be defined for an EBLUP as it was
discussed in section \ref{sec:eblup_mspe}. @Dat05 study the MSPE estimation 
using the results of @Pra90 for different estimations of the variance component 
$\sigre$. They compare the method of moment estimator by @Pra90 with the
original estimator by @Fay79 and the maximum likelihood estimator by @Dat00. The
main finding is that with respect to the MSPE estimation the estimator by @Fay79
performs best overall.

A jackknife MSPE estimator for linear mixed models was introduced by @Jia02 and
later subject to several refinements; see for example @Che03. @Che08 then
introduced a jackknife estimator based on the results of @Jia02 and explicitly
targeted the MSPE estimation of the prediction under a Fay-Herriot model. They
find satisfying results for the MSPE estimation using their method, however, the
conclusion with respect to the estimation of $\sigre$ is not as clear as by
@Dat05, when they compare a method of moment estimator to the method proposed by
@Fay79.

A different line of discussion is stimulated by the fact that it is assumed that
the sampling variances, $\sige$, are known parameters. In practice this is not 
the case and these parameters are estimated using the sample data. This can mean
that they are themselfes direct estimators; but if a direct mean is considered 
unreliable then its variance estimation cannot be considered reliable. @Fay79 
used generalized variance functions - see @Wol07[pp. 272 ff] for a discussion of
these methods - instead of direct estimators. @Mai14 suggest instead to shrink 
both means and variances to account for the possibility of unstable direct
variance estimates and also provide an estimator of the MSPE of the predictions.
This approach is based on a Baysian modelling strategy. @You06 provide results 
for the case that direct variance estimates are used in a hirarchical Bayes 
approach and can account for that extra variability in the MSPE estimation. MSPE
estimators for an EBLUP based prediction using estimated sampling variances can
be found in @Wan03 and @Riv03. @Wan03 derive an MSPE using asymptotic properties
of the EBLUP. @Riv03 instead extend the results of @Pra90 and add an extra term
to the MSPE estimator to account for the additional variability associated with 
the estimation of direct sampling variances.

### Discussion

From a practical point of view the assumption of known sampling variances under
the model is not plausible. Hence these variances are subject to estimation but
are treated as known constants. Some approaches to face this problem have been
reviewed in section \ref{sec:theory_sae_fh_mspe} because in principle this is
mostly related to an underestimation of the true uncertainty of the predictions.
Another dimension which was for example addressed by @Mai14 is the instability
of predictions when very heterogenous sampling variances are observed, by
shrinking both means and variances. Another approach is to stabalize the
sampling variances by using generalized variance functions or other smoothing
techniques. Albeit these parameters are assumed to be known they can have a
large impact on the validity of domain predictions. In section (?) I will show
how the robust FH model relates to this discussion in terms of stability and
MSPE.

The response variable, $\tilde{y}_i$, denotes a direct estimator. This is, of
course, not necessarily the sample mean but can be any other statistic. An
important feature of this statistic that it is design unbiased. So in principle
it can be a direct design based estimator such as the HT estimator. However, it
is assumed that the sampling errors are independent. This can be a plausible
assumption under simple random sampling but is not necessarily plausible under
an informative sampling design. With respect to the response variable it also
needs to be noted that very often not the direct estimator itself but a suitable
transformation is used. @Fay79 log\hyp{}transoform the direct estimator and
suggest to use a transformation such that a normal distribution is plausible,
i.e. is supported by the observed data. Hence several suggestions have been made
how to optimally transform the response variable and how to asses the estimation
of the MSPE of the back transformed domain prediction; see for example @Slu06.
@Sug15 review and introduce several parametric transformations for the FH model
and also discuss the possibility of MSPE estimation.

One of the main motivations to consider area level models is the availability of
data. Especially with census or administrative data, it may not be possible to
give unit level information directly to the analyst due to reasons of
confidentiality. Thus only aggregates, i.e. direct domain estimations, are
reported. Despite the availability of information there are other reasons to
consider. One is the integration of sampling weights which is in general not
directly feasible in model based methodology. Area level models present a way to
at least incorporate design weights into the direct estimation and then have a
design unbiased estimator on the area level. Other reasons can be practical
reasons, e.g. the reduction of computational demands because area level data 
often means a dramitic reduction in the number of observations, hence more 
complex variance structures can be modelled with less computational effort.
Other reasons are discussed in more detail in @Nam15 who consider different
scenarios for the availibility of auxiliary information, e.g. unit and area
level variables and contextual variables. Their findings show that overall unit
level models have more potential to reduce the MSPE of domain predictions, which
is not supprising as parameter estimation under a unit level model uses more 
information and thus is more precise. This leaves the availibility of data as 
the main reason to consider area level models.

Area level models in small area estimation have found many use cases and the
Fay-Herriot model explicitly is subjuct to numerous extensions. See for example
@Cle14 for a review in desease mapping; @Gua15 for a review of methods used for 
  poverty mapping; and @Ben16 for an EBLUP under a multivariate FH model. A
  comprehensive review of extension can be found in @Rao03[pp. 153 ff] and
  @Rao15[pp ?]. The following section reviews some advances for incorporating
  structures in space and time into the estimation process. Some of these
  results in addition to a robust extension is then subject of chapter (?).


## Spatial and Temporal Fay-Harriot Models

This section reviews spatial and temporal extensions to the FH model. These
extensions are again subject in chapter (?) where they are then combined with
robust estimation methodology which is reviewed in section (?). It is in
principle intuitive that if historical data, e.g. yearly repeated surveys, is
available it should be used. From a mixed model perspective we can modify two
components, the random effects or the errer term. The use of correlated random
effects with modified variance structures to allow for spatial, temporal or
spatio-temporal effects may be beneficial with respect to domain predictions.
However, an early extension by @Cho89 allows for an autoregressive process in
the error term, which in the context of the FH model is one way to account for
correlated sampling errors. @Rao94 then introduced a model to use auto correlated random effects to borrow strength for domain predictions. 

More measurements will improve everything. The sampling model can then be defined as
$$
\tilde{y}_{it} = \theta_{it} + e_{it}
$$
with $i = 1, \dots, D$ and $t = 1, \dots, T$ where $D$ and $T$ are the total 
number of areas and time periods respectively. 


Here $e_{it} \sim\Distr{N}(0, 
\sigma^2_{eit})$ are independent with known variances $\sigma_{eit}^2$. The
model error is composed of a spatial autoregressive process of order 1 (SAR(1))
and an autoregressive process of order 1 (AR(1)):
$$
\theta_{it} = \mat{x}^\top_{it}\beta + u_{1i} + u_{2it}
$$
where $u_{1i}$ and $u_{2it}$ follow a SAR(1) and AR(1) respectively:
$$
u_{1i} = \rho_1 \sum_{l\neq i}w_{il} u_{1l} + \epsilon_{1i}
$$
where $|\rho_1| < 1$ and $\epsilon_{1i} \sim \Distr{N}(0, \sigma_1^2)$ are
i.i.d. with $i = 1,\dots, D$. $w_{il}$ are the elements of $\mat{W}$ which is the
row standardized proximity matrix $\mat{W}^0$. The elements in $\mat{W}^0$ are equal to 1 if
areas are neighboured and 0 otherwise, thus the dimension of $\mat{W}^0$ is $D \times D$. As stated above $u_{2it}$ follows an
AR(1):
$$
u_{2it} = \rho_2 u_{2i, t-1} + \epsilon_{2it}
$$
where $|\rho_2| < 1$ and $\epsilon_{2it} \sim \Distr{N}(0, \sigma_2^2)$ are
i.i.d. with $i = 1, \dots, D$ and $t = 1, \dots, T$. Note that $u_{1i}$ and
$u_{2it}$ and $e_{it}$ are independent and the sampling error variance
parameters are assumed to be known. The model can then be stated as:
$$
\mat{y} = \mat{X}\beta + \mat{Z}\mat{u} + \mat{e},
$$
where $\mat{y}$ is the $DT\times 1$ vector containing $y_{it}$ as elements,
$\mat{X}$ is the $DT \times P$ design matrix containing the vectors
$\mat{x}^\top_{it}$ as rows, $\mat{u}$ is the $(D + DT) \times 1$ vector of model
errors and $\mat{e}$ the $DT \times 1$ vector of sampling errors $e_{it}$.
Note that $\mat{u} = (\mat{u}_1^\top, \mat{u}_2^\top)$ where the $D\times 1$ vector $\mat{u}_1$
and $DT \times 1$ vector $\mat{u}_2$ have $\mat{u}_{1i}$ and $\mat{u}_{2it}$ as elements
respectively. Furthermore $\mat{Z} = (\mat{Z}_1, \mat{Z}_2)$ has
dimension $DT \times (D+DT)$, where $\mat{Z}_1 = \mat{I}_D \otimes
\mat{1}_T$ ($\mat{I}_D$ denotes a $D\times D$ identity matrix and
$\mat{1}_T$ a $1 \times T$ vector of ones) has dimension $DT \times D$ and
$\mat{Z}_2$ is a $DT \times DT$ identity matrix.

Concerning the variance of $\mat{y}$ first consider the distributions of all
error components. $\mat{e} \sim \Distr{N}(\mat{0}, \mat{V}_e)$ where
$\mat{V}_e$ is a diagonal matrix with the known $\sigma^2_{eit}$ on the main
diagonal. $\mat{u} \sim \Distr{N}(\mat{0}, \mat{V}_u(\delta))$ with
the block diagonal covariance matrix $\mat{V}_u(\delta) =
\text{diag}(\sigma_1^2\Omega_1(\rho_1), \sigma_2^2\Omega_2(\rho_2))$ where
$\delta = (\sigma_1^2, \rho_1, \sigma_2^2, \rho_2)$.

$$
\Omega_1(\rho_1) = \left((\mat{I}_D - \rho_1\mat{W})^\top (\mat{I}_D - \rho_1\mat{W})\right)^{-1}
$$
and follows from the SAR(1) process in the model errors. $\Omega_2(\rho_2)$ has
a block diagonal structure with $\Omega_{2i}(\rho_2)$ denoting the blocks where
the definition of $\Omega_{2i}(\rho_2)$ follows from the AR(1) process:
  $$
    \Omega_{2i}(\rho_2) = \frac{1}{1-\rho_2^2}
    \left(
      \begin{matrix}
      1 & \rho_2 & \cdots & \rho_2^{T-2}& \rho_2^{T-1}\\
      \rho_2 & 1 & & & \rho_2^{T-2} \\
      \vdots & & \ddots & & \vdots \\
      \rho_2^{T-2} &&& 1 & \rho_2 \\
      \rho_2^{T-1} & \rho_2^{T-2} & \cdots & \rho_2 & 1\\
      \end{matrix}
      \right)_{T\times T}
  $$
The variance of $\mat{y}$ can thus be stated as:
$$
\mathbb{V}(\mat{y}) = \mat{V}(\delta) = \mat{Z}\mat{V}_u(\delta)\mat{Z}^\top + \mat{V}_e
$$
The BLUE of $\beta$ and BLUP of $\delta$ can be stated as \citep[see][]{Hen75}:
$$
\tilde{\beta}(\delta) = \left(\mat{X}^\top \mat{V}^{-1}(\delta) \mat{X} \right)^{-1} \mat{X}^\top \mat{V}^{-1}(\delta) \mat{y}
$$
$$
\tilde{\mat{u}}(\delta) = \mat{V}_u(\delta) \mat{Z}^\top \mat{V}^{-1}(\delta) \left(\mat{y} - \mat{X}\tilde{\beta}(\delta)\right)
$$
Hence the BLUP of $\mat{u}_1$ and $\mat{u}_2$ can be stated as:
$$
\tilde{\mat{u}}_1(\delta) = \sigma_1^2 \Omega_1(\rho_1) \mat{Z}^\top \mat{V}^{-1}(\delta) \left(\mat{y} - \mat{X}\tilde{\beta}(\delta)\right)
$$
$$
\tilde{\mat{u}}_2(\delta) = \sigma_2^2 \Omega_2(\rho_2) \mat{Z}^\top \mat{V}^{-1}(\delta) \left(\mat{y} - \mat{X}\tilde{\beta}(\delta)\right)
$$
Estimating $\delta$ leads to the EBLUE for $\beta$ and EBLUPs for $\mat{u}_1$ and
$\mat{u}_2$, hence an predictor for the area characteristic $\theta_{it}$ is given by:
$$
\hat{\theta}^{STFH}_{it} = \mat{x}_{it}^\top \hat{\beta} + \hat{\mat{u}}_{1i} + \hat{\mat{u}}_{2it}
$$
@Mar13 use a restricted maximum likelihood method to estimate $\delta$
  independently of $\beta$. An open question is if this approach can be applied
  for the robust spatio-temporal model. Thus we will continue with the
  discussion of robust small area methods.


- Porter, Holan, Wikle, Cressie (2014): Spatial Fay-Herriot models for small area estimation with functional covariates
- López-Vizcaíno, Lombardía and Morales (2015): Small area estimation of labour force indicators under multinomial models with correlated time and area effects
- Rao and Yu (1994): Small-Area Estimation by Combining Time-Series and Cross-Sectional Data
- Benedetti, Pratesi and Salvati (2013): Local stationarity in small area estimation models

- Benavent and Morales (2015): Multivariate Fay-Herriot models for small area estimation



