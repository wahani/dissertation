\chapter{Simulation Tools for Small Area Estimation}

\begin{flushright}{\slshape    
Instead of imagining that our main task is \\ 
to instruct a computer what to do, \\
let us concentrate rather on explaining \\
to human beings what we want a computer to do.} \\ \medskip
--- \textcite[p.99]{Knu92}
\end{flushright}

# Outline

In this Chapter I want to present a framework for simulation studies within the 
SAE field. The framework is implemented in the `R`-package `saeSim` \citep{War15}.
At this point it is important to note that the following content has in part
been published as an article: \citetitle{War16a} -- @War16a. Here a shorter 
version of this article is presented. Content which is related to a general 
introduction to the SAE field is omitted; in contrast to the article code 
examples use the SAE methods implemented in `saeRobust` \citep{War16} instead of
`sae` \citep{Mol15}. Furthermore text passages have been changed to integrate
the content into the general line of argumentation of this Thesis.

The set of tools contributed by the package `saeSim` are motivated by providing 
an infrastructure which makes it easier to reproduce results of model and design
based simulation studies. Reproducible research aims at the availability of the 
full academic research, which is the article combined with the full 
computational environment, including data and source code. Open source tools 
like the `R`\hyp{}language \citep{R} and \LaTeX \space can be used in tight 
integration to combine the statistical analysis with the written words in an 
article. This can be achieved by using tools like `knitr` \citep{Xie13}, 
`sweave` \citep{Lei02}, and more recently `rmarkdown` \citep{All14}. Such tools 
can assist in making research more reproducible. The tools provided by `saeSim`
aim at simplyfying the process of writing the source code for a simulation 
study; in the context of reproducible research this may prove to be useful in
the development of script files as well as in the setting of the tools discussed
earlier.

Real data is often very sensitive and governed by strict confidentiality rules. 
Synthetic data generation mechanisms can be used to provide safe data which can 
be made publicly available -- for a more thorough discussion see @Alf11 and
@Kol11. @Bur14 interpret this as an open research philosophy. Such synthetic
data sets can be used to test newly proposed statistical methods in a
close-to-reality framework. In general, simulation studies in statistics can be
divided into two concepts:

- Design-based: The simulation study is based on true or synthetic data of a 
fixed population. Then, samples are selected repeatedly from the underlying 
finite population and different estimation methods are applied in each 
replication. The estimates so obtained are compared to the true values of the 
population, for instance, in terms of relative bias (RBIAS) and relative root
mean squared prediction error (RRMSPE).
- Model-based: The simulation study uses data directly drawn from a model.
In each iteration the population is generated from a model and a sample is
selected according to a specific sampling scheme. The sample is used to estimate
the target statistic for which quality measures (like the RBIAS and RRMSPE)
are derived.

Further discussion regarding model- and design-based simulations can be found in
@Mue03, @Sal10, and @Alf10.

A closely related software packages in the `R`-language is `simFrame`
\citep{Alf10} which helps to configure simulation studies in a reproducible
environment. It includes a wide range of features -- like data generation,
sampling schemes, outlier contamination mechanisms, and missing values -- and
has been originally developed for simulations in the context of survey
statistics but is now designed to be as general as possible \citep{Alf10}. The
package `simPop` \citep{Alf14} supports the generation of synthetic population
data. This can be a suitable environment in scenarios where the reproducibility
of results and confidentiality issues play an important role.

In contrast to `simFrame` the package `saeSim` is different in 
design and has a focus on assisting applications in the SAE field. Most 
importantly it is based on a framework which is mapped into the software
package. This framework defines the overall structure and aims to unify the
shared elements between simulation studies. A simulation is here defined
as a stream of data to be manipulated in a sequence of steps. Furthermore it
provides the definition of the interface between these steps. The package 
`saeSim` maps this framework into the `R`-language and combines it with commonly 
used facilities in this context; e.g. tools for data generation, sampling, and a
link to the parallel computing capabilities in `R`.

The framework is more concretely presented in the following Section 
\ref{sec:saeSim_framework}. This presentation is followed by code examples in 
Section \ref{sec:saeSim_examples} implementing a simple model-based simulation
study to illustrate the capabilities of the package. Section
\ref{sec:saeSim_discussion} concludes this Chapter with a brief discussion.


# A Simulation Framework
\label{sec:saeSim_framework}

The framework strongly relies on the idea to describe a simulation as a process 
of data manipulation. Independent of simulation studies, @Wic15 and @Wic16
promote this idea by providing tools for cleaning and transforming data. In
those frameworks every defined function takes a `data.frame` as input and 
returns it modified. This leads to a natural connection between all defined 
functions since the result of one function can be directly passed to the next as
an argument. The symbioses of these packages with the pipe operator, `%>%`, from
the package `magrittr` \citep{Bac14} only emphasizes this process.

In `saeSim` this approach is extended to simulation studies in the SAE field.
The main focus is the description of a simulation as a process of data
manipulation. Each step in this process can be defined as a self contained
component -- a function in `R` -- and thus can be easily replaced, extended, and
reused.

Simulation studies address three different levels; these are the population, the
sample, and data on aggregated level. Figure \ref{fig:flowdiagram} illustrates 
these levels. The left column describes the steps of data manipulation, the 
right column presents the function names in `saeSim` to define the corresponding
steps. The *population\hyp{}level* defines the data on which a study is 
conducted and may be based on real population data, a synthetic population, or 
randomly generated variates from a model. In the context of a design\hyp{}based 
simulation a simulation study is based on true or synthetic data of *one* 
population. In model\hyp{}based simulations the population can be randomly drawn
from a population model in each repetition.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figs/flowdiagram.pdf}
\caption[Process of Simulation]{\label{fig:flowdiagram}Process of Simulation -- Left column are the steps in a simulation. Right column are the corresponding function names to represent those steps in \texttt{R}.} 
\end{figure}

The scope of the framework is not to opt for viewpoints. The aim is to 
incorporate the different simulation concepts in a common framework. The *base* 
(first component in Figure \ref{fig:flowdiagram}) of a simulation study is a
data table; here the question is whether this data is *fixed* or *random* over 
simulations. Or from a more technical point of view, is the data generation (the second step in Figure \ref{fig:flowdiagram}) repeated in each repetition 
or omitted in the study. Depending on the choice of a fixed or random population
it is necessary to re\hyp{}compute the population target statistics like domain
means and variances, or other statistics of interest (third component in
Figure \ref{fig:flowdiagram}).

The *sample\hyp{}level* is necessary when domain predictions are conducted for 
unit-level models. Independently of how the population is treated - whether as 
fixed or random - this phase consists of two steps: Firstly, drawing a sample 
according to a specific sampling scheme. Secondly, conducting computations on 
the samples (fourth and fifth component in Figure \ref{fig:flowdiagram}). Given
a sample, small area methods are applied. Of interest are, for instance, 
estimated model parameters, domain predictions, or measures of uncertainty for
the estimates.

Since the sample\hyp{}level is necessary when unit level models are applied, the
*aggregate\hyp{}level* is conducted when area level models are applied (the 
seventh and last component in Figure \ref{fig:flowdiagram}). Area level models in
SAE typically only use information available for domains -- in
contrast to units. Thus the question for simulation studies for area level 
methods is whether the data is generated on unit level and is used after the 
aggregation (sixth component in Figure \ref{fig:flowdiagram}) or whether the
data is generated directly on area level, i.e. drawn from an area level model. 
Depending on whether or not unit-level data and sampling are part of the 
simulation process, the aggregate-level follows the generation of the population
or is based on the aggregated sample.

Depending on the topic of research, some steps in this simulation framework can 
be more relevant than others. The framework defines a complete list of steps
which may be of interest. Single components may be omitted if not relevant in
specific applications. For example *data generation* is not relevant if we have
population data, or the *sample\hyp{}level* is not used when the sample is
directly drawn from the model.

Seen this way, `saeSim` maps the different steps into `R`. Two layers with 
separate responsibilities need to be discussed. The first is *how* different 
simulation components can be combined, and the second is *when* they are 
applied. Regarding the first, in `saeSim` an emphasis is put on the interface of
each component. To be precise, functions are used which take a `data.frame` as 
argument and have a `data.frame` as return value. The return value of one 
component is the input of the next. This definition of interfaces is used for 
all existing tools in `saeSim`. The second column in Figure 
\ref{fig:flowdiagram} shows how the different steps in a simulation can be 
accessed. It is important to note that the functions in Figure 
\ref{fig:flowdiagram} control the process, the second layer, i.e. *when* 
components are applied. Each of these functions take a simulation setup object 
to be modified and a function with the discussed interface as arguments. To 
illustrate these implementation details the following Section gives some code 
examples to implement a model based simulation.


# Code Examples
\label{sec:saeSim_examples}

We present two applications of `saeSim`, one model-based simulation in Section
\ref{sec:csModel} and a design-based simulation in Section \ref{sec:csDesign}.
First, though, we introduce some basic functionalities as the pipe operator
(`%>%`) needs some explanation. The pipe operator is designed to make otherwise
nested expressions more readable as a line can be read from left to right,
instead from inside out \citep{bache14}. As a simple example see the following
lines which are equivalent with respect to their functionality:

```{r eval = FALSE}
library("magrittr")
colMeans(matrix(rnorm(10), ncol = 2))
rnorm(10) %>% matrix(ncol = 2) %>% colMeans
```

In `saeSim`, we rely on this operator. Although all functions can be used
without it, we strongly recommend its usage. The following example shows some of
the aspects of the package:

```{r eval = TRUE, echo=FALSE, message=FALSE}
library("saeSim")
```

```{r eval=FALSE}
library("saeSim")
setup1 <- sim_base_lm() %>% sim_sample(sample_number(5))
setup2 <- sim_base_lm() %>% sim_sample(sample_fraction(0.05))
```

Without knowing anything about the setup defined in `sim_base_lm` we notice that
`setup1` and `setup2` only differ in the applied sampling scheme. `sim_sample`
is responsible as a control when a function is applied (after the
population-level) and `sample_number(5)` and `sample_fraction(0.05)` define the
explicit way of drawing samples. Separating the responsibility of each component
into what is applied and when it is applied makes it possible to add new
components to any step in the process. The composition of a simulation in that
manner will focus on the definition of components and hide control structures.
Any function can be passed to `sim_sample` which has a `data.frame` both as
input and as return value. The only responsibility of that function is to draw a
sample, which makes it easy to find, understand and reuse when published. The
operator `%>%` is used to add new components to the setup.

In the following we show one way how to construct a simulation in a model-based
setting. The aim is to estimate the domain predictions under a FH model.
Involved components are *data generation* and *computing on aggregated data*
(cf. Figure \ref{fig:flowdiagram}). The first step is to generate the data under
the model:

$$ y_i = 100 + 2 \cdot x_i + v_i + e_i,$$

where $x_i \stackrel{iid}{\sim} N(0, 4^2)$, $v_i \stackrel{iid}{\sim} N(0, 1)$
and $e_i \stackrel{indep}{\sim} N(0, \sigma_i^2)$ with $\sigma_i^2 = 0.1, 0.2,
\dots, 4$ and $i = 1, \dots, 40$ as index for the domains. $x_i$, $v_i$ and
$e_i$ are independent from each other. The area-level data for the simulation is
generated in each Monte Carlo replication.

In this case the *base-component* is a data frame with an id variable named
`idD` and constructed with the function `base_id`. Any random number generator
in `R` can be used. However, we have normally distributed variates, for which
some predefined functions are available in the package. For the reproducibility
of the following results we also set the seed to 1. The seed is not part of a
simulation setup in `saeSim` but needs to be defined by the researcher.

```{r}
set.seed(1)
setup <- base_id(nDomains = 40, nUnits = 1) %>% 
  sim_gen_x(mean = 0, sd = 4) %>%
  sim_gen_v(mean = 0, sd = 1)
setup
```

Note that if you print a simulation setup to the console, as in the above
example, one simulation run is performed and only the first rows (the head) of
the resulting data table are printed. This enables interactivity with the object
itself; however, it hides the fact that the setup object is a collection of
functions to be called. In this model the error component $e_i$ has different
variances which is not covered by a predefined function. Thus, as a *generator
component*, we define a function which takes a `data.frame` as input and returns
it after adding a variable named `vardir` with the variances and the variable
`e` with the generated random numbers:

```{r}
gen_e <- function(dat) {
  dat$vardir <- seq(0.1, 4, length.out = nrow(dat))
  dat$e <- rnorm(nrow(dat), sd = sqrt(dat$vardir))
  dat
}
setup <- setup %>% sim_gen(gen_e)
setup
```

The last step in data generation is to construct the response variable which is
named `y` and is added to the data. Furthermore, we add the *true* area
statistic under the model to the data:

```{r}
setup <- setup %>% 
  sim_resp_eq(y = 100 + 2 * x + v + e) %>%
  sim_comp_pop(comp_var(trueStat = y - e))
```

To add the area-level predictions from a Fay-Herriot model we need to define
another component. The function takes a `data.frame` as input and returns the
modified version. For the estimation of the EBLUP under the FH model we use the
function `eblupFH` from the package `sae` \citep{molina13}. To avoid naming
conflicts between the dependencies of `sae` and the package `dplyr`
\citep{wickham14a} we make use of the double colon operator in order to call the
function `eblupFH` without attaching the package. Hence we define a function
named `comp_FH` and add it to the process:

```{r}
comp_FH <- function(dat) {
  modelFH <- sae::eblupFH(y ~ x, vardir, data = dat)
  dat$FH <- as.numeric(modelFH$eblup)
  dat
}
setup <- setup %>% sim_comp_agg(comp_FH)
setup
```

The object `setup` stores all necessary information to run one iteration of the
simulation. In the following $R = 100$ repetitions are performed. The result is
a `list` of `data.frame`s. The function `rbind_all` from the package `dplyr` is
used to combine the resulting `list`:

```{r}
library("dplyr")
simResults <- sim(setup, R = 100) %>% rbind_all
simResults %>% select(idD, idR, simName, trueStat, y, FH)
```

An additional variable `idR` is automatically added as an ID-variable for the
iteration as well as a variable `simName` to distinguish between scenarios. In
`saeSim` we do not provide further tools to process the resulting data as there
are many tools readily available in `R`. In the design-based scenario we show
how to process the result data into graphs with only a few lines of code.

# Discussion
\label{sec:saeSim_discussion}
