# Robust Methods in Small Area Estimation

The quality of predictions using model based methods are strongly connected to
their distributional assumptions. Severe problems with these methods can already
arise when single outlying observations are present in the data. Hence some
discussion and progress has been made with respect to methods robust against
outliers.

In the broader context of this subfield within SAE this thesis relies strongly 
on the results of @Sin09 who proposed robust estimation quations for linear 
mixed models. The proposition of @Sin09 has a focus on the unit level 
model \eq{eq:bhf}, however the methodology builds on linear mixed models where 
\eq{eq:bhf} can be derived as a special case of these models. Also the basic
area level model \eq{eq:fh} can be framed in this context and the estimation
equations can be adapted; this is also pointed out in @Sin08 and @Rao15[pp. 146
ff]. Using these results for area level models is again addressed in chapter
\ref{chap:rfh} where robust extensions of the FH model are proposed.

In the following the main results around the methodology introduced by @Sin09 
are reviewed. Together with the models reviewed in \ref{sec:area} these results 
are the basis for the extensions proposed in this thesis. The section is 
structured as follows: section \ref{sec:robustee} reviews the methodology around
robust estimation equations; in section \ref{sec:robustbc} these results are
extended to allow for a correction of the bias asscociated with these type of
robust predictions; in section \ref{sec:robustmse} then different possibilities
to estimate the MSPE of the area level predictions are described; and in section
\ref{sec:robustdiscussion} these results are embedded in a broader context of
proposed robust methods in the SAE field.

## Robust Estimation Equations
\label{sec:robustee}

Section \ref{sec:blup} reviewed the domain prediction under linear mixed models.
Key to these predictions are the BLUE given by \eq{eq:blue} and the BLUP given
by \eq{eq:blupre}. These estimators can be derived based on the log-likelihood
of the joint density of $\mat{y}$ and $\mat{\re}$ of \eq{eq:lmm}
$$
\mat{y} = \mat{X} \beta + \mat{Z} \mat{\re} + \mat{e}
$$
when we assume normality of the distribution of $\mat{\re}$ and $\mat{e}$. 
Remember that the model now can be stated as $\mat{y} \sim 
\Distr{N}(\mat{X}\beta, \mat{V})$ where $\mat{V} = 
\mat{Z}\mat{V}_\re\mat{Z}^\top + \mat{V}_e$. The first derivatives of the 
log-likelihood with respect to $\beta$ and $\mat{\re}$ for given variance 
parameters $\delta$ then lead to the so-called *mixed model equations*:
\empty{
\begin{align}
\mat{X}^\top\mat{V}_e^{-1}\left(\mat{y}-\mat{X}\beta - \mat{Z} \mat{\re}\right) &= 0 \label{eq:mmeb} \\
\mat{Z}^\top \mat{V}_e^{-1} \Paran{\mat{y}-\mat{X}\beta - \mat{Z} \mat{\re}} - \mat{V}_\re^{-1} \mat{\re} &= 0. \label{eq:mmere}
\end{align}
}@Hen63 showed that the solutions to \eq{eq:mmeb} and \eq{eq:mmere} are
identical to the BLUE and BLUP defined in \eq{eq:blue} and \eq{eq:blupre}
respectively.

@Fel86 studied the robust estimation of $\beta$ and $\mat{\re}$ by modifying
  \eq{eq:mmeb} and \eq{eq:mmere} to restrict the influence of outlying
  observations. He suggests to use an influence function to restrict the
  influence of the residuals in \eq{eq:mmeb} and \eq{eq:mmere} modifying them
  into:
\empty{
\begin{align}
\mat{X}^\top\mat{V}_e^{-\half}\psi\left(\mat{V}_e^{-\half} \Paran{\mat{y}-\mat{X}\beta - \mat{Z} \mat{\re}}\right) &= 0 \label{eq:rmmeb} \\
\intertext{and}
\mat{Z}^\top \mat{V}_e^{-\half} \psi \Paran{\mat{V}_e^{-\half} \Paran{\mat{y}-\mat{X}\beta - \mat{Z} \mat{\re}}} - \mat{V}_\re^{-\half} \psi\Paran{\mat{V}_\re^{-\half}\mat{\re}} &= 0 \label{eq:rmmere}
\end{align}
}where $\psi(\cdot)$ is a bounded monotonous influence function. @Fel86 suggests
to use Huber's influence function (cf. @Hub64) where $\psi_b(x) = x 
\min\Paran{1, \frac{b}{|x|}}$ for a given tuning canstant $b$ where a common 
choice for $b$ is 1.345. @Rao03[102] suggests to use a robust version of 
Henderson's (cf. @Hen50) estimation equations for the unknown parameters in 
$\delta$; remember that $V = V(\delta)$ where $\delta$ denotes the vector of
unknown variance parameters.

Building on these results @Sin09 proposed a similar approach in that they use 
*robustified* estimation equations. In contrast to the approach by @Fel86 these 
equations are derived from the marginal model of \eq{eq:lmm} and specifically
for $\beta$ and $\delta$; thus the first derivatives of the marginal
log-likelihood with respect to $\beta$ and $\delta$ are:
\empty{
\begin{align*}
\mat{X}^\top\mat{V}^{-1}\left(\mat{y}-\mat{X}\beta\right) &= 0 \\
\intertext{and}
\left(\mat{y} - \mat{X}\beta\right)^\top\mat{V}^{-1}          %(y-Xb)'V^-1
\frac{\partial\mat{V}}{\partial\delta_l}                      % dV/dx
\mat{V}^{-1}\left(\mat{y} - \mat{X}\beta\right) -             %V^-1 (y-Xb)
\tr\left(\mat{V}^{-1}\frac{\partial\mat{V}}{\partial\delta_l}\right) &= 0
\end{align*}
}for $l = 1, \dots, Q$ where $Q$ denotes the total number of variance parameters
in $\delta$. These estimation equations are now modified to restrict the
influence of outlying observations similar to \eq{eq:rmmeb} in that the
residuals have bounded influence using, also, an influence function denoted by
$\psi(\cdot)$:
\empty{
\begin{align}
\label{eq:reeb}
\mat{X}^\top\mat{V}^{-1} \mat{U}^{\frac{1}{2}} \psi(\mat{r}) & = 0 \\
\label{eq:reed}
\psi(\mat{r})^\top\mat{U}^{\frac{1}{2}}\mat{V}^{-1} 
\frac{\partial\mat{V}}{\partial\delta_l}
\mat{V}^{-1}\mat{U}^{\frac{1}{2}} \psi(\mat{r}) - \tr\left(K\mat{V}^{-1}\frac{\partial\mat{V}}{\partial\delta_l}\right) & = 0 
\end{align}
}where $K = \Exp{E}(\psi_b^2(z)) \mat{I}_n$ is a diagonal matrix of the same 
order as $\mat{V}$ with $z$ following a standard normal distribution; and 
$\mat{r} = \mat{r}(\beta) = \mat{U}^{-\half} (\mat{y} - \mat{X}\beta)$ denotes
the vector of the standardised residuals and $\mat{U} = \mat{U}(\delta)$ is the
matrix containing the diagonal elements of $\mat{V}$.

In order to find solutions for $\beta$ and $\delta$ based on the robust
estimation equations \eq{eq:reeb} and \eq{eq:reed} @Sin09 proposed
Newton-Raphson algorithms. @Sch12 and @Schoch12 have reported numerical problems
with this approach and different alternative solutions have been investigated.
@Sch12 proposed to find the minimum of the squared estimation equations in
\eq{eq:reed} for all variance parameters, which was further optimized by @Aue15
with stable and satisfying results. @Schoch12 uses instead an iteratively
re-weighted least squares (IRWLS) algorithm for a solution of \eq{eq:reeb}
which also yields satisfying results. Another proposition has been made by
@Chat12 which is to derive a system of fixed point equations based on
\eq{eq:reed} and she derives a stable algorithm - using a fixed point algorithm -
for $\delta$. 

The main advantage of these alternatives may be that they do not rely on the 
derivatives of the estimation equations, which was already a known concern in 
the work of @Sin09. See @Ken80[chapter 11] and @Thi88[chapter 4] for 
comprehensive overviews for solving ML based robust estimation equations, who 
also suggest the IRWLS algorithm as an alternative to the NR algorithm for this
very reason. 

With given robust estimates for $\beta$ and $\delta$ @Sin09 solve the robust 
mixed model equation \eq{eq:rmmere} proposed by @Fel86 in order to derive a 
robust EBLUP (REBLUP). Although they also suggest to use a NR algorithm which is
developed using a Taylor series expansion, the numerical solution appears to be
less problematic. This is indicated by @Sch12, @Aue15, and @Chat12 whose main
concern are stable solutions for $\delta$. On the other hand @Schoch12 suggests
to find solutions based on a robust version of a method of moments estimator and
a similar suggestion can be found in @Rao15[196] which is to use a robust
version of \eq{eq:blupre}:
\empty{
\begin{align}
\hat{\mat{\re}}^\psi = \mat{V}_u\mat{Z}^\top\mat{V}^{-1}\mat{U}^\half\psi(\mat{r}) \label{eq:reblupre}
\end{align}
}where the notation is the same as before with the only difference being that 
$\hat{\mat{\re}}^\psi$ now depends on the robust parameter estimates 
$\hat{\beta}^\psi$ and $\hat{\delta}^\psi$ for which different solutions exist 
as discussed before. From a computational point of view this has the obvious 
advantage that no iterative algorithm is needed. @Rao15[196] note that a 
disadvantage may be that \eq{eq:reblupre} depends on the composite error - 
$\mat{Z}\mat{\re} + \mat{e}$ - of \eq{eq:lmm} whereas in \eq{eq:rmmere} the 
influence function is applied only to $\mat{e}$. However, an empirical
investigation of this issue can not be found in the literature at this time.

Given the robust estimates $\hat{\beta}^\psi$, $\hat{\delta}^\psi$, and $\hat{\mat{\re}}^\psi$ @Sin09 derive the REBLUP for area level means under the BHF model \eq{eq:bhf}. The REBLUP is then a robust variation of \eq{eq:bhfeblup} and can be stated as:
\empty{
\begin{align*}
\hat{\theta}_i^{RBHF} = N_i^{-1} \Paran{\sum_{j \in S_i} y_{ij} + 
  \sum_{j\in R_i}(\sij{\mat{x}}^\top\hat{{\beta}}^\psi + \si{\hat{\re}^\psi})}
\end{align*}
}



## Bias Correction
\label{sec:robustbc}

- Tzavidis and Chambers (2005): BIAS ADJUSTED SMALL AREA ESTIMATION WITH M-QUANTILE MODELS
- Jiongo, Haziza and Duchesne (2013): Controlling the bias of robust small-area estimators


## Mean Squared Error Estimation
\label{sec:robustmse}

### Bootstrap

\label{sec:theory_sae_robust_bootstrap}

- Jiiongo, Nguimekeu (2014): Bootstrapping Mean Squared Errors of Robust Small
Area Estimators


### Pseudo Linearization
\label{sec:theory_sae_robust_mspe}

@Cha11 and @Cha14 deal with the estimation of the MSE of robust area predictors
  in the context of Small Area Estimation. In this section I review their
  results. Later in section ? I will, firt, adapt their findings to estimate the
  MSE of the robustified Fay Herriot model, and second use the linearization of
  robust mixed models to derive a fixed point algorithm to find solutions for
  the model parameters.

The central idea is to formulate the RBLUP as wigthed sum of the response vector:

$$
\si{\theta}^{RBLUP} = \sum_{j \in s} \sij{w}^{RBLUP} \sij{y} = \Paran{\mat{w}_{is}^{RBLUP}}^\top \mat{y}_s
$$

where

$$
\Paran{\mat{w}_{is}^{RBLUP}}^\top = N_i^{-1} \Paran{\mat{1}_s^\top + (N_i - n_i) \Paran{\bar{x}_{ir}^\top \mat{A}_s + \bar{z}_{ir}^\top \mat{B}_s \Paran{\mat{I}_s - \mat{X}_s \mat{A}_s }}}
$$

  and

$$
\mat{A}_s = \Paran{\mat{X}_s^\top \mat{V}_s^{-1} \mat{U}_s^\frac{1}{2} \mat{W}_{1s} \mat{U}_s^{-\frac{1}{2}} \mat{X}_s }^{-1} \mat{X}_s^\top \mat{V}_s^{-1} \mat{U}_s^\frac{1}{2} \mat{W}_{1s} \mat{U}_s^{-\frac{1}{2}}
$$

with

$$
\mat{W}_{1s} = \Diag{w_j}_{n \times n}
$$

and

$$
w_{1j} = \frac{\psi\Paran{ U_j^{-\frac{1}{2}} \Paran{ y_j - x_j^\top\hat{\beta}^\psi } }}{ U_j^{-\frac{1}{2}} \Paran{ y_j - x_j^\top\hat{\beta}^\psi }}
$$

$$
  \mat{B}_s =
  \Paran{
    \mat{Z}_s^\top \mat{V}_{es}^{-\frac{1}{2}} \mat{W}_{2s} \mat{V}_{es}^{-\frac{1}{2}} \mat{Z}_s +
      \mat{V}_u^{-\frac{1}{2}} \mat{W}_{3s} \mat{V}_u^{-\frac{1}{2}}
    }^{-1}
\mat{Z}_s^\top \mat{V}_e^{-\frac{1}{2}} \mat{W}_{2s} \mat{V}_e^{-\frac{1}{2}}
$$

  with $\mat{W}_{2s}$ as diagonal matrix with ith component:

  $$
  w_{2i} =
  \frac{
    \psi\Paran{\Paran{\sigma^\psi_{e, i}}^{-1} \Paran{y_i - x_i^\top \hat{\beta}^\psi - \hat{u}^\psi_i}}
  }{
    \Paran{\sigma^\psi_{e, i}}^{-1} \Paran{y_i - x_i^\top \hat{\beta}^\psi - \hat{u}^\psi_i}
  }
$$

and with $\mat{W}_{3s}$ as $\Paran{m \times m}$ diagonal matrix with ith component:

  $$
  w_{3i} = \frac{
    \psi\Paran{\Paran{\sigma_u^\psi}^{-1} \hat{u}^\psi_i}
  }{
    \Paran{\sigma_u^\psi}^{-1} \hat{u}^\psi_i
  }
$$

This all assumes known variance parameters. When the variance parameters are
unknown, they are estimated and instead of $\mat{w}_{is}^{RBLUP}$ we have to use
$\mat{w}_{is}^{REBLUP}$. Then the estimator of the conditional MSE is given by:

$$
\widehat{MSE}\Paran{\si{\widehat{\theta}}^{REBLUP}} = 
  \widehat{\Exp{V}}\Paran{\si{\widehat{\theta}}^{REBLUP}} + 
  \widehat{\Exp{B}}\Paran{\si{\widehat{\theta}}^{REBLUP}}^2
$$

$$
\widehat{\Exp{V}}\Paran{\si{\widehat{\theta}}^{REBLUP}} =
  N_i^{-2} \sum_{j \in s} \Paran{a_{ij}^2 + \Paran{N_i - n_i} n^{-1}} \lambda_j^{-1}\Paran{y_j - \hat{\mu}_j}^2
$$

with 

$$
a_{ij} = N_i w_{ij}^{REBLUP} - I\Paran{j \in i}
$$

and

$$
\widehat{\Exp{B}}\Paran{\si{\widehat{\theta}}^{REBLUP}} =
  \sum_{j \in s} w_{ij}^{REBLUP} \hat{\mu}_j - N_i^{-1} \sum_{j \in \Paran{r_i \cup s_i}} \hat{\mu}_j
$$

Note that $\hat{\mu}_j$ is an unbiased estimator of the the conditional expectation $\mu_j = \Exp{E}\Paran{y_j | \mat{x}_j, \mat{\re}^\psi}$. $\lambda_j = 1 - 2\phi_{jj} + \sum_{k \in s}\phi^2_{kj}$.

## Discussion
\label{sec:robustdiscussion}

A reported problem with this method is the numerical stability of the parameter 
estimates. The robust estimation equations are solved using a Newton-Raphson 
algorithm; Hence @Schoch12 proposes a different numerical solution for these 
optimisation problems. @Schoch12 uses similar robust estimation equations for
the model parameters of the fixed effects part but suggests to use a variation
of Huber's proposal 2 (cf. @Hub64) to estimate the variance parameter of the
random effects. Extensions based on the results of @Sin09 can also be found in
@Sch12 and @Sch14 who use the results for the BHF model with spatially 
correlated random effects similar to \eq{eq:sfh}.


Three lines of discussion on robust small area estimation can be identified. The use of alter First, if the distributional assumption - often a
Gaussian distribution - appears to be implausible then intuition demands that it
be replaced. This often leads to the use of non-symmetric or heavy-tailed 
distributions for the model error or the random effect. Due to their flexibility
Bayesian modelling strategies are often used in this context; see for example
@Dat95 and @Bel06. Secondly, methods are applied which are *naturally* more
robust against outlying observations. @Tza10 model a global conditional median,
or more generally a quantile, instead of a mean. The third approach is to remain
with the original model or method and *robustify* the estimation equations. In
this context @Sin09 develop a robustified EBLUP; @Bea09 refer to a winzorisation
of the Horvitz-Thompson estimator; and @Bea04 introduces a robust extension to 
generalized regression estimation.

- Sinha and Rao (2008): Robust Small Area Estimation Under Unit Level Models
- Richardson and Welsh (1995): Robust Restricted Maximum Likelihood in Mixed Linear Models
- Yau and Kuk (2002): Robust Estimation in Generalized Linear Mixed Models
- Hulliger (2010): Simple and Robust Estimators for Sampling
- Lange, Little and Taylor (1989): Robust Statistical Modeling Using the t Distribution
- Peel and McLachlan (2000): Robust mixture modelling using the t distribution
- Bell and Huang (2006): Using the t-distribution to Deal with Outliers in Small Area Estimation
- Huang and Bell (): Using the t-distribution in Small Area Estimation: An Application to SAIPE State Poverty Models
- Schoch (2012): Robust Unit-Level Small Area Estimation: A Fast Algorithm for Large Datasets
- Xie, Raghunathan and Lepkowski (2007): Estimation of the proportion of overweight individuals in small areas - a robust extension of the Fay-Herriot model
- Gershunskaya (2010): Robust Small Area Estimation Using a Mixture Model
- Datta and Lahiri (1995): Robust Hierarchical Bayes Estimation of Small Area Characteristics in the Presence of Covariates and Outliers
- Rao, Sinha and Dumitrescu (2014): Robust Small area estimation under semi-parametric mixed models
- Salvati et.al. (2012): Small area estimation via M-quantile geographically weighted regression
- Salvati et.al. (2009): Spatial M-Quantile Modles for Small Area Estimation
- Shmid and Münnich (2012): Spatial robust small area estimation
- Tzavidis et.al. (2010): Robust Prediction of Small Area Means and Distributions
- Gervini / Yohai (2002): A Class of Robust and Fully Efficient Regression Estimators
- Pra99


